package openai

import (
	"context"
	"encoding/json"
	"errors"
	"net/http"
)

// Chat message role defined by the OpenAI API.
const (
	ChatMessageRoleSystem    = "system"
	ChatMessageRoleUser      = "user"
	ChatMessageRoleAssistant = "assistant"
	ChatMessageRoleFunction  = "function"
	ChatMessageRoleTool      = "tool"
)

const chatCompletionsSuffix = "/chat/completions"
const supervisorSuffix = "/supervisor"

var (
	ErrChatCompletionInvalidModel       = errors.New("this model is not supported with this method, please use CreateCompletion client method instead") //nolint:lll
	ErrChatCompletionStreamNotSupported = errors.New("streaming is not supported with this method, please use CreateChatCompletionStream")              //nolint:lll
	ErrContentFieldsMisused             = errors.New("can't use both Content and MultiContent properties simultaneously")
)

type Hate struct {
	Filtered bool   `json:"filtered" bson:"filtered"`
	Severity string `json:"severity,omitempty" bson:"severity"`
}
type SelfHarm struct {
	Filtered bool   `json:"filtered" bson:"filtered"`
	Severity string `json:"severity,omitempty" bson:"severity"`
}
type Sexual struct {
	Filtered bool   `json:"filtered" bson:"filtered"`
	Severity string `json:"severity,omitempty" bson:"severity"`
}
type Violence struct {
	Filtered bool   `json:"filtered" bson:"filtered"`
	Severity string `json:"severity,omitempty" bson:"severity"`
}

type ContentFilterResults struct {
	Hate     Hate     `json:"hate,omitempty" bson:"hate"`
	SelfHarm SelfHarm `json:"self_harm,omitempty" bson:"self_harm"`
	Sexual   Sexual   `json:"sexual,omitempty" bson:"sexual"`
	Violence Violence `json:"violence,omitempty" bson:"violence"`
}

type PromptAnnotation struct {
	PromptIndex          int                  `json:"prompt_index,omitempty" bson:"prompt_index"`
	ContentFilterResults ContentFilterResults `json:"content_filter_results,omitempty" bson:"content_filter_results"`
}

type ImageURLDetail string

const (
	ImageURLDetailHigh ImageURLDetail = "high"
	ImageURLDetailLow  ImageURLDetail = "low"
	ImageURLDetailAuto ImageURLDetail = "auto"
)

type ChatMessageImageURL struct {
	URL    string         `json:"url" bson:"url,omitempty"`
	Detail ImageURLDetail `json:"detail" bson:"detail"`
}

type ChatMessagePartType string

const (
	ChatMessagePartTypeText     ChatMessagePartType = "text"
	ChatMessagePartTypeImageURL ChatMessagePartType = "image_url"
)

type ChatMessagePart struct {
	Type     ChatMessagePartType  `json:"type,omitempty" bson:"type"`
	Text     string               `json:"text,omitempty" bson:"text"`
	ImageURL *ChatMessageImageURL `json:"image_url,omitempty" bson:"image_url"`
}

type ChatCompletionGuardRail struct {
	Type     string   `json:"type" bson:"type"`
	Category []string `json:"category" bson:"category"`
}

type ChatCompletionAnalysis struct {
	Expertise string `json:"expertise" bson:"expertise"`
	Sentiment string `json:"sentiment" bson:"sentiment"`
	Reasoning string `json:"reasoning" bson:"reasoning"`
}

type ChatCompletionMessage struct {
	Role         string `json:"role" bson:"role"`
	Content      string `json:"content" bson:"content"`
	Reasoning    string `json:"reasoning,omitempty" bson:"reasoning"`
	MultiContent []ChatMessagePart

	// This property isn't in the official documentation, but it's in
	// the documentation for the official library for python:
	// - https://github.com/openai/openai-python/blob/main/chatml.md
	// - https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb
	Name string `json:"name,omitempty" bson:"name"`

	FunctionCall *FunctionCall `json:"function_call,omitempty" bson:"function_call"`

	// For Role=assistant prompts this may be set to the tool calls generated by the model, such as function calls.
	ToolCalls []ToolCall `json:"tool_calls,omitempty" bson:"tool_calls"`

	// For Role=tool prompts this should be set to the ID given in the assistant's prior request to call a tool.
	ToolCallID string `json:"tool_call_id,omitempty" bson:"tool_call_id"`
	// A simple string describing the guard rails that were triggered.
	GuardRails *ChatCompletionGuardRail `json:"guard,omitempty" bson:"guard"`       // DEPRECATED
	Analysis   *ChatCompletionAnalysis  `json:"analysis,omitempty" bson:"analysis"` // DEPRECATED
}

func (m ChatCompletionMessage) MarshalJSON() ([]byte, error) {
	if m.Content != "" && m.MultiContent != nil {
		return nil, ErrContentFieldsMisused
	}
	if len(m.MultiContent) > 0 {
		msg := struct {
			Role         string                   `json:"role"`
			Content      string                   `json:"-"`
			Reasoning    string                   `json:"reasoning,omitempty"`
			MultiContent []ChatMessagePart        `json:"content,omitempty"`
			Name         string                   `json:"name,omitempty"`
			FunctionCall *FunctionCall            `json:"function_call,omitempty"`
			ToolCalls    []ToolCall               `json:"tool_calls,omitempty"`
			ToolCallID   string                   `json:"tool_call_id,omitempty"`
			GuardRails   *ChatCompletionGuardRail `json:"guard,omitempty"`
			Analysis     *ChatCompletionAnalysis  `json:"analysis,omitempty"`
		}(m)
		return json.Marshal(msg)
	}
	msg := struct {
		Role         string                   `json:"role"`
		Content      string                   `json:"content"`
		Reasoning    string                   `json:"reasoning,omitempty"`
		MultiContent []ChatMessagePart        `json:"-"`
		Name         string                   `json:"name,omitempty"`
		FunctionCall *FunctionCall            `json:"function_call,omitempty"`
		ToolCalls    []ToolCall               `json:"tool_calls,omitempty"`
		ToolCallID   string                   `json:"tool_call_id,omitempty"`
		GuardRails   *ChatCompletionGuardRail `json:"guard,omitempty"`
		Analysis     *ChatCompletionAnalysis  `json:"analysis,omitempty"`
	}(m)
	return json.Marshal(msg)
}

func (m *ChatCompletionMessage) UnmarshalJSON(bs []byte) error {
	msg := struct {
		Role         string `json:"role"`
		Content      string `json:"content"`
		Reasoning    string `json:"reasoning,omitempty"`
		MultiContent []ChatMessagePart
		Name         string                   `json:"name,omitempty"`
		FunctionCall *FunctionCall            `json:"function_call,omitempty"`
		ToolCalls    []ToolCall               `json:"tool_calls,omitempty"`
		ToolCallID   string                   `json:"tool_call_id,omitempty"`
		GuardRails   *ChatCompletionGuardRail `json:"guard,omitempty"`
		Analysis     *ChatCompletionAnalysis  `json:"analysis,omitempty"`
	}{}
	if err := json.Unmarshal(bs, &msg); err == nil {
		*m = ChatCompletionMessage(msg)
		return nil
	}
	multiMsg := struct {
		Role         string `json:"role"`
		Content      string
		Reasoning    string                   `json:"reasoning,omitempty"`
		MultiContent []ChatMessagePart        `json:"content"`
		Name         string                   `json:"name,omitempty"`
		FunctionCall *FunctionCall            `json:"function_call,omitempty"`
		ToolCalls    []ToolCall               `json:"tool_calls,omitempty"`
		ToolCallID   string                   `json:"tool_call_id,omitempty"`
		GuardRails   *ChatCompletionGuardRail `json:"guard,omitempty"`
		Analysis     *ChatCompletionAnalysis  `json:"analysis,omitempty"`
	}{}
	if err := json.Unmarshal(bs, &multiMsg); err != nil {
		return err
	}
	*m = ChatCompletionMessage(multiMsg)
	return nil
}

type ToolCall struct {
	// Index is not nil only in chat completion chunk object
	Index    *int         `json:"index,omitempty" bson:"index"`
	ID       string       `json:"id" bson:"id"`
	Type     ToolType     `json:"type" bson:"type"`
	Function FunctionCall `json:"function" bson:"function"`
}

type FunctionCall struct {
	Name string `json:"name,omitempty" bson:"name,omitempty"`
	// call function with arguments in JSON format
	Arguments string `json:"arguments,omitempty" bson:"arguments,omitempty"`
}

type ChatCompletionResponseFormatType string

const (
	ChatCompletionResponseFormatTypeJSONObject ChatCompletionResponseFormatType = "json_object"
	ChatCompletionResponseFormatTypeText       ChatCompletionResponseFormatType = "text"
)

type ChatCompletionResponseFormat struct {
	Type ChatCompletionResponseFormatType `json:"type,omitempty"`
}

// ChatCompletionRequest represents a request structure for chat completion API.
type ChatCompletionRequest struct {
	Model            string                        `json:"model" bson:"model"`
	Messages         []ChatCompletionMessage       `json:"messages" bson:"messages"`
	MaxTokens        int                           `json:"max_tokens,omitempty" bson:"max_tokens"`
	Temperature      float32                       `json:"temperature" bson:"temperature"`
	TopP             float32                       `json:"top_p,omitempty" bson:"top_p"`
	N                int                           `json:"n,omitempty" bson:"n"`
	Stream           bool                          `json:"stream,omitempty" bson:"stream"`
	Stop             []string                      `json:"stop,omitempty" bson:"stop"`
	PresencePenalty  float32                       `json:"presence_penalty,omitempty" bson:"presence_penalty"`
	ResponseFormat   *ChatCompletionResponseFormat `json:"response_format,omitempty" bson:"response_format"`
	Seed             *int                          `json:"seed,omitempty" bson:"seed"`
	FrequencyPenalty float32                       `json:"frequency_penalty,omitempty" bson:"frequency_penalty"`
	// LogitBias is must be a token id string (specified by their token ID in the tokenizer), not a word string.
	// incorrect: `"logit_bias":{"You": 6}`, correct: `"logit_bias":{"1639": 6}`
	// refs: https://platform.openai.com/docs/api-reference/chat/create#chat/create-logit_bias
	LogitBias map[string]int `json:"logit_bias,omitempty" bson:"logit_bias"`
	// LogProbs indicates whether to return log probabilities of the output tokens or not.
	// If true, returns the log probabilities of each output token returned in the content of message.
	// This option is currently not available on the gpt-4-vision-preview model.
	LogProbs bool `json:"logprobs,omitempty" bson:"logprobs"`
	// TopLogProbs is an integer between 0 and 5 specifying the number of most likely tokens to return at each
	// token position, each with an associated log probability.
	// logprobs must be set to true if this parameter is used.
	TopLogProbs int    `json:"top_logprobs,omitempty" bson:"top_logprobs"`
	User        string `json:"user,omitempty" bson:"user"`
	// Deprecated: use Tools instead.
	Functions []FunctionDefinition `json:"functions,omitempty" bson:"functions"`
	// Deprecated: use ToolChoice instead.
	FunctionCall any    `json:"function_call,omitempty" bson:"function_call"`
	Tools        []Tool `json:"tools,omitempty" bson:"tools"`
	// This can be either a string or an ToolChoice object.
	ToolChoice any `json:"tool_choice,omitempty" bson:"tool_choice"`
	// Options for streaming response. Only set this when you set stream: true.
	StreamOptions *StreamOptions `json:"stream_options,omitempty" bson:"stream_options"`
	// Disable the default behavior of parallel tool calls by setting it: false.
	ParallelToolCalls any `json:"parallel_tool_calls,omitempty" bson:"parallel_tool_calls"`
}
type StreamOptions struct {
	// If set, an additional chunk will be streamed before the data: [DONE] message.
	// The usage field on this chunk shows the token usage statistics for the entire request,
	// and the choices field will always be an empty array.
	// All other chunks will also include a usage field, but with a null value.
	IncludeUsage bool `json:"include_usage,omitempty" bson:"include_usage"`
}

type ToolType string

const (
	ToolTypeFunction         ToolType = "function"
	ToolTypeBuiltInFunctiopn ToolType = "built_in_function"
)

type Tool struct {
	Type     ToolType            `json:"type" bson:"type"`
	Function *FunctionDefinition `json:"function,omitempty" bson:"function"`
}

type ToolChoice struct {
	Type     ToolType     `json:"type" bson:"type"`
	Function ToolFunction `json:"function,omitempty" bson:"function"`
}

type ToolFunction struct {
	Name string `json:"name" bson:"name"`
}

type FunctionDefinition struct {
	Name        string `json:"name" bson:"name"`
	Description string `json:"description,omitempty" bson:"description"`
	// Parameters is an object describing the function.
	// You can pass json.RawMessage to describe the schema,
	// or you can pass in a struct which serializes to the proper JSON schema.
	// The jsonschema package is provided for convenience, but you should
	// consider another specialized library if you require more complex schemas.
	Parameters any `json:"parameters" bson:"parameters"`
}

// Deprecated: use FunctionDefinition instead.
type FunctionDefine = FunctionDefinition

type TopLogProbs struct {
	Token   string  `json:"token" bson:"token"`
	LogProb float64 `json:"logprob" bson:"logprob"`
	Bytes   []byte  `json:"bytes,omitempty" bson:"bytes"`
}

// LogProb represents the probability information for a token.
type LogProb struct {
	Token   string  `json:"token" bson:"token"`
	LogProb float64 `json:"logprob" bson:"logprob"`
	Bytes   []byte  `json:"bytes,omitempty" bson:"bytes"` // Omitting the field if it is null
	// TopLogProbs is a list of the most likely tokens and their log probability, at this token position.
	// In rare cases, there may be fewer than the number of requested top_logprobs returned.
	TopLogProbs []TopLogProbs `json:"top_logprobs" bson:"top_logprobs"`
}

// LogProbs is the top-level structure containing the log probability information.
type LogProbs struct {
	// Content is a list of message content tokens with log probability information.
	Content []LogProb `json:"content" bson:"content"`
}

type FinishReason string

const (
	FinishReasonStop          FinishReason = "stop"
	FinishReasonLength        FinishReason = "length"
	FinishReasonFunctionCall  FinishReason = "function_call"
	FinishReasonToolCalls     FinishReason = "tool_calls"
	FinishReasonContentFilter FinishReason = "content_filter"
	FinishReasonNull          FinishReason = "null"
)

func (r FinishReason) MarshalJSON() ([]byte, error) {
	if r == FinishReasonNull || r == "" {
		return []byte("null"), nil
	}
	return []byte(`"` + string(r) + `"`), nil // best effort to not break future API changes
}

type ChatCompletionChoice struct {
	Index   int                   `json:"index" bson:"index"`
	Message ChatCompletionMessage `json:"message" bson:"message"`
	// FinishReason
	// stop: API returned complete message,
	// or a message terminated by one of the stop sequences provided via the stop parameter
	// length: Incomplete model output due to max_tokens parameter or token limit
	// function_call: The model decided to call a function
	// content_filter: Omitted content due to a flag from our content filters
	// null: API response still in progress or incomplete
	FinishReason FinishReason         `json:"finish_reason" bson:"finish_reason"`
	LogProbs     *LogProbs            `json:"logprobs,omitempty" bson:"logprobs"`
	TaskResults  TaskResultCollection `json:"task_results,omitempty" bson:"task_results"`
}

// ChatCompletionResponse represents a response structure for chat completion API.
type ChatCompletionResponse struct {
	ID                string                 `json:"id" bson:"id"`
	Object            string                 `json:"object" bson:"object"`
	Created           int64                  `json:"created" bson:"created"`
	Model             string                 `json:"model" bson:"model"`
	Choices           []ChatCompletionChoice `json:"choices" bson:"choices"`
	Usage             Usage                  `json:"usage" bson:"usage"`
	SystemFingerprint string                 `json:"system_fingerprint" bson:"system_fingerprint"`

	httpHeader
}

// CreateChatCompletion â€” API call to Create a completion for the chat message.
func (c *Client) CreateChatCompletion(
	ctx context.Context,
	request ChatCompletionRequest,
) (response ChatCompletionResponse, err error) {
	if request.Stream {
		err = ErrChatCompletionStreamNotSupported
		return
	}

	urlSuffix := chatCompletionsSuffix
	if !checkEndpointSupportsModel(urlSuffix, request.Model) {
		err = ErrChatCompletionInvalidModel
		return
	}

	req, err := c.newRequest(ctx, http.MethodPost, c.fullURL(urlSuffix, request.Model), withBody(request))
	if err != nil {
		return
	}

	err = c.sendRequest(req, &response)
	return
}

func (c *Client) CreateSupervisorCompletion(
	ctx context.Context,
	request SupervisorRequest,
) (response SupervisorResponse, err error) {
	urlSuffix := supervisorSuffix
	if !checkEndpointSupportsModel(urlSuffix, request.Model) {
		err = ErrChatCompletionInvalidModel
		return
	}

	req, err := c.newRequest(ctx, http.MethodPost, c.fullURL(urlSuffix, request.Model), withBody(request.ToNeolangInput()))
	if err != nil {
		return
	}

	err = c.sendRequest(req, &response)
	return
}
